{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16f8e14-a858-4143-8272-9b56e8c62b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What does R-squared represent in a regression model\n",
    "R-squared measures the proportion of the variance in the dependent variable explained by the model. It ranges from 0 to 1.\n",
    "\n",
    "2. What are the assumptions of linear regression\n",
    "\n",
    "Linearity\n",
    "\n",
    "Independence of errors\n",
    "\n",
    "Homoscedasticity\n",
    "\n",
    "Normality of residuals\n",
    "\n",
    "No multicollinearity\n",
    "\n",
    "3. What is the difference between R-squared and Adjusted R-squared\n",
    "\n",
    "R-squared: Increases with more features, even if not useful.\n",
    "\n",
    "Adjusted R-squared: Penalizes unnecessary features; adjusts R-squared based on model complexity.\n",
    "\n",
    "4. Why do we use Mean Squared Error (MSE)\n",
    "MSE measures the average squared difference between predicted and actual values. It‚Äôs a common loss function for regression models.\n",
    "\n",
    "5. What does an Adjusted R-squared value of 0.85 indicate\n",
    "It means 85% of the variance in the target is explained by the model, adjusted for the number of predictors.\n",
    "\n",
    "6. How do we check for normality of residuals in linear regression\n",
    "\n",
    "Histogram or Q-Q plot\n",
    "\n",
    "Shapiro-Wilk test or Kolmogorov-Smirnov test\n",
    "\n",
    "7. What is multicollinearity, and how does it impact regression\n",
    "Multicollinearity occurs when predictors are highly correlated. It makes coefficient estimates unstable and inflates standard errors.\n",
    "\n",
    "8. What is Mean Absolute Error (MAE)\n",
    "MAE is the average of absolute differences between predictions and actual values. It‚Äôs less sensitive to outliers than MSE.\n",
    "\n",
    "9. What are the benefits of using an ML pipeline\n",
    "\n",
    "Organizes steps in a workflow\n",
    "\n",
    "Reduces errors\n",
    "\n",
    "Ensures reproducibility\n",
    "\n",
    "Simplifies hyperparameter tuning and deployment\n",
    "\n",
    "10. Why is RMSE considered more interpretable than MSE\n",
    "RMSE is in the same units as the target variable, while MSE is in squared units.\n",
    "\n",
    "11. What is pickling in Python, and how is it useful in ML\n",
    "Pickling is a way to serialize (save) Python objects, such as trained models, for reuse.\n",
    "\n",
    "12. What does a high R-squared value mean\n",
    "It indicates that the model explains a large proportion of the variance in the target variable.\n",
    "\n",
    "13. What happens if linear regression assumptions are violated\n",
    "It can lead to biased or inefficient estimates, invalid hypothesis tests, and unreliable predictions.\n",
    "\n",
    "14. How can we address multicollinearity in regression\n",
    "\n",
    "Remove correlated features\n",
    "\n",
    "Use regularization (Ridge, Lasso)\n",
    "\n",
    "Apply dimensionality reduction (PCA)\n",
    "\n",
    "15. How can feature selection improve model performance in regression analysis\n",
    "By removing irrelevant or redundant features, we reduce overfitting and improve interpretability.\n",
    "\n",
    "16. How is Adjusted R-squared calculated\n",
    "\n",
    "Adjusted¬†R\n",
    "2\n",
    "=\n",
    "1\n",
    "‚àí\n",
    "(\n",
    "(\n",
    "1\n",
    "‚àí\n",
    "ùëÖ\n",
    "2\n",
    ")\n",
    "√ó\n",
    "(\n",
    "ùëõ\n",
    "‚àí\n",
    "1\n",
    ")\n",
    "ùëõ\n",
    "‚àí\n",
    "ùëù\n",
    "‚àí\n",
    "1\n",
    ")\n",
    "Adjusted¬†R \n",
    "2\n",
    " =1‚àí( \n",
    "n‚àíp‚àí1\n",
    "(1‚àíR \n",
    "2\n",
    " )√ó(n‚àí1)\n",
    "‚Äã\n",
    " )\n",
    "where \n",
    "ùëõ\n",
    "n = number of observations, \n",
    "ùëù\n",
    "p = number of predictors.\n",
    "\n",
    "17. Why is MSE sensitive to outliers\n",
    "MSE squares the errors, so large errors (from outliers) have a disproportionate effect.\n",
    "\n",
    "18. What is the role of homoscedasticity in linear regression\n",
    "Homoscedasticity means constant variance of residuals. Its absence (heteroscedasticity) can lead to biased standard errors.\n",
    "\n",
    "19. What is Root Mean Squared Error (RMSE)\n",
    "RMSE is the square root of MSE. It represents the typical error in the units of the target variable.\n",
    "\n",
    "20. Why is pickling considered risky\n",
    "Pickled files can execute arbitrary code during loading, posing security risks if the source is untrusted.\n",
    "\n",
    "21. What alternatives exist to pickling for saving ML models\n",
    "\n",
    "Joblib (for large NumPy arrays)\n",
    "\n",
    "ONNX (for model interoperability)\n",
    "\n",
    "HDF5 (Keras models)\n",
    "\n",
    "TorchScript for PyTorch models\n",
    "\n",
    "SavedModel for TensorFlow\n",
    "\n",
    "22. What is heteroscedasticity, and why is it a problem\n",
    "Heteroscedasticity is when residuals have non-constant variance, leading to inefficient estimates and unreliable hypothesis tests.\n",
    "\n",
    "23. How can interaction terms enhance a regression model's predictive power\n",
    "Interaction terms capture combined effects of variables that aren‚Äôt explained by their individual contributions. This can improve model accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f796ee94-b8a6-4af2-b00e-7717a8e2d283",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Visualize Residuals (Diamonds Dataset)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load diamonds dataset\n",
    "diamonds = sns.load_dataset('diamonds')\n",
    "X = diamonds[['carat', 'depth', 'table']]\n",
    "y = diamonds['price']\n",
    "\n",
    "# Fit model\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "residuals = model.resid\n",
    "\n",
    "# Plot residuals\n",
    "sns.histplot(residuals, kde=True)\n",
    "plt.title(\"Residuals Distribution\")\n",
    "plt.show()\n",
    "2. Calculate MSE, MAE, RMSE for Linear Regression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "y_true = [3, 5, 7, 9]\n",
    "y_pred = [2.5, 5.2, 6.8, 9.1]\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"MSE: {mse}, MAE: {mae}, RMSE: {rmse}\")\n",
    "3. Check Linear Regression Assumptions\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load dataset\n",
    "tips = sns.load_dataset('tips')\n",
    "X = tips[['total_bill', 'size']]\n",
    "y = tips['tip']\n",
    "\n",
    "# Linear model\n",
    "X = sm.add_constant(X)\n",
    "model = sm.OLS(y, X).fit()\n",
    "residuals = model.resid\n",
    "fitted = model.fittedvalues\n",
    "\n",
    "# Linearity\n",
    "sns.scatterplot(x=fitted, y=y)\n",
    "plt.title(\"Linearity Check\")\n",
    "plt.show()\n",
    "\n",
    "# Homoscedasticity\n",
    "sns.scatterplot(x=fitted, y=residuals)\n",
    "plt.title(\"Residuals vs Fitted\")\n",
    "plt.show()\n",
    "\n",
    "# Multicollinearity\n",
    "corr = tips[['total_bill', 'size', 'tip']].corr()\n",
    "print(corr)\n",
    "4. ML Pipeline with Feature Scaling and Regression\n",
    "sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X, y = load_boston(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "pipelines = {\n",
    "    'linear': Pipeline([('scaler', StandardScaler()), ('lr', LinearRegression())]),\n",
    "    'rf': Pipeline([('scaler', StandardScaler()), ('rf', RandomForestRegressor())])\n",
    "}\n",
    "\n",
    "for name, pipe in pipelines.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    print(f\"{name} R2: {r2_score(y_test, y_pred):.2f}\")\n",
    "5. Simple Linear Regression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([3, 5, 7, 9])\n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    "print(\"Coefficient:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"R-squared:\", model.score(X, y))\n",
    "6. Linear Regression (Tips Dataset)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "tips = sns.load_dataset('tips')\n",
    "X = tips[['total_bill']]\n",
    "y = tips['tip']\n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    "\n",
    "# Plot\n",
    "sns.scatterplot(x='total_bill', y='tip', data=tips)\n",
    "plt.plot(tips['total_bill'], model.predict(X), color='red')\n",
    "plt.show()\n",
    "7. Simple Linear Regression (Synthetic Data)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.random.rand(50, 1) * 10\n",
    "y = 2 * X.flatten() + 5 + np.random.randn(50)\n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, model.predict(X), color='red')\n",
    "plt.show()\n",
    "8. Pickle a Linear Regression Modelpython\n",
    "\n",
    "import pickle\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1], [2], [3]])\n",
    "y = np.array([2, 4, 6])\n",
    "model = LinearRegression().fit(X, y)\n",
    "\n",
    "with open('linear_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "9. Polynomial Regression (Degree 2)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = np.linspace(0, 10, 50).reshape(-1, 1)\n",
    "y = 3 * X.flatten()**2 + 2*X.flatten() + 5 + np.random.randn(50)*10\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "model = LinearRegression().fit(X_poly, y)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, model.predict(X_poly), color='red')\n",
    "plt.show()\n",
    "10. Generate Synthetic Data & Linear Regression\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.flatten() + 7 + np.random.randn(100)\n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    "print(\"Coefficient:\", model.coef_[0])\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "11. Compare Polynomial Regression Models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = np.linspace(0, 10, 50).reshape(-1, 1)\n",
    "y = X.flatten()**3 - 5*X.flatten()**2 + X.flatten() + np.random.randn(50)*10\n",
    "\n",
    "for degree in [1, 2, 3, 4]:\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression().fit(X_poly, y)\n",
    "    y_pred = model.predict(X_poly)\n",
    "    print(f\"Degree {degree} R2: {r2_score(y, y_pred):.2f}\")\n",
    "12. Simple Linear Regression with Two Features\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.random.rand(100, 2) * 10\n",
    "y = 3 * X[:, 0] + 5 * X[:, 1] + 7 + np.random.randn(100)\n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"R-squared:\", model.score(X, y))\n",
    "13. Visualize Linear Regression Line (Synthetic Data)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.linspace(0, 10, 50).reshape(-1, 1)\n",
    "y = 2 * X.flatten() + 3 + np.random.randn(50)\n",
    "\n",
    "model = LinearRegression().fit(X, y)\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, model.predict(X), color='red')\n",
    "plt.title(\"Linear Regression Fit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e73fa-1375-493e-b188-b8597f35086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "14. Write a Python script that uses the Variance Inflation Factor (VIF) to check for multicollinearity in a dataset with multiple features.\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "df = pd.DataFrame(X, columns=[f'Feature_{i}' for i in range(5)])\n",
    "\n",
    "# Calculate VIF\n",
    "vif = pd.DataFrame()\n",
    "vif['Feature'] = df.columns\n",
    "vif['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "print(vif)\n",
    "15. Write a Python script that generates synthetic data for a polynomial relationship (degree 4), fits a polynomial regression model, and plots the regression curve.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(42)\n",
    "X = np.sort(np.random.rand(100, 1) * 10, axis=0)\n",
    "y = 5 + 2*X + 0.5*X**2 - 0.3*X**3 + 0.1*X**4 + np.random.randn(100, 1) * 10\n",
    "\n",
    "# Polynomial Regression\n",
    "poly = PolynomialFeatures(degree=4)\n",
    "X_poly = poly.fit_transform(X)\n",
    "model = LinearRegression().fit(X_poly, y)\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.plot(X, model.predict(X_poly), color='red')\n",
    "plt.title('Polynomial Regression (Degree 4)')\n",
    "plt.show()\n",
    "16. Write a Python script that creates a machine learning pipeline with data standardization and a multiple linear regression model, and prints the R-squared score.\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=5, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "r2_score = pipeline.score(X_test, y_test)\n",
    "print(\"R-squared Score:\", r2_score)\n",
    "17. Write a Python script that performs polynomial regression (degree 3) on a synthetic dataset and plots the regression curve.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.sort(5 * np.random.rand(80, 1), axis=0)\n",
    "y = np.sin(X).ravel() + np.random.normal(0, 0.1, size=X.shape[0])\n",
    "\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_poly = poly.fit_transform(X)\n",
    "model = LinearRegression().fit(X_poly, y)\n",
    "\n",
    "plt.scatter(X, y, color='blue')\n",
    "plt.plot(X, model.predict(X_poly), color='red')\n",
    "plt.title('Polynomial Regression (Degree 3)')\n",
    "plt.show()\n",
    "18. Write a Python script that performs multiple linear regression on a synthetic dataset with 5 features. Print the R-squared score and model coefficients.\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "model = LinearRegression().fit(X, y)\n",
    "print(\"R-squared:\", model.score(X, y))\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "19. Write a Python script that generates synthetic data, fits a linear regression model, and visualizes the data points along with the regression line.\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = np.random.rand(50, 1) * 10\n",
    "y = 3 * X.squeeze() + 7 + np.random.randn(50) * 2\n",
    "model = LinearRegression().fit(X, y)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X, model.predict(X), color='red')\n",
    "plt.title('Linear Regression Line')\n",
    "plt.show()\n",
    "20. Create a synthetic dataset with 3 features and perform multiple linear regression. Print the model's R-squared score and coefficients.\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=5, random_state=42)\n",
    "model = LinearRegression().fit(X, y)\n",
    "print(\"R-squared:\", model.score(X, y))\n",
    "print(\"Coefficients:\", model.coef_)\n",
    "21. Write a Python script that demonstrates how to serialize and deserialize machine learning models using joblib instead of pickling.\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "import joblib\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
    "model = LinearRegression().fit(X, y)\n",
    "\n",
    "# Serialize\n",
    "joblib.dump(model, 'linear_model.joblib')\n",
    "\n",
    "# Deserialize\n",
    "loaded_model = joblib.load('linear_model.joblib')\n",
    "print(\"R-squared:\", loaded_model.score(X, y))\n",
    "22. Write a Python script to perform linear regression with categorical features using one-hot encoding. Use the Seaborn 'tips' dataset.\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "tips = sns.load_dataset('tips')\n",
    "X = tips[['total_bill', 'sex', 'smoker', 'day']]\n",
    "y = tips['tip']\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(drop='first'), ['sex', 'smoker', 'day'])\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(X, y)\n",
    "print(\"R-squared:\", pipeline.score(X, y))\n",
    "23. Compare Ridge Regression with Linear Regression on a synthetic dataset and print the coefficients and R-squared score.\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=5, random_state=42)\n",
    "\n",
    "lr = LinearRegression().fit(X, y)\n",
    "ridge = Ridge(alpha=1.0).fit(X, y)\n",
    "\n",
    "print(\"Linear Regression R-squared:\", lr.score(X, y))\n",
    "print(\"Ridge Regression R-squared:\", ridge.score(X, y))\n",
    "print(\"Linear Coefficients:\", lr.coef_)\n",
    "print(\"Ridge Coefficients:\", ridge.coef_)\n",
    "24. Write a Python script that uses cross-validation to evaluate a Linear Regression model on a synthetic dataset.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples=100, n_features=3, noise=5, random_state=42)\n",
    "model = LinearRegression()\n",
    "scores = cross_val_score(model, X, y, cv=5, scoring='r2')\n",
    "\n",
    "print(\"Cross-validated R-squared scores:\", scores)\n",
    "print(\"Mean R-squared:\", scores.mean())\n",
    "25. Write a Python script that compares polynomial regression models of different degrees and prints the R-squared score for each.\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.squeeze()**2 + 3 * X.squeeze() + 5 + np.random.randn(100) * 10\n",
    "\n",
    "for degree in range(1, 5):\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    model = LinearRegression().fit(X_poly, y)\n",
    "    y_pred = model.predict(X_poly)\n",
    "    print(f'Degree {degree} R-squared:', r2_score(y, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
